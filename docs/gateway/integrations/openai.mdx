---
title: OpenAI
description: Integrate Uno LLM Gateway with OpenAI SDKs using the Responses API
---

Uno LLM Gateway supports OpenAI's **Responses API** for text generation, images, tool calling, and reasoning. You can use **any OpenAI SDK** by simply pointing it to Uno's gateway endpoint. This allows you to leverage Uno's features like virtual keys, provider management, and observability while using your existing OpenAI code.

## Overview

The Uno LLM Gateway provides an endpoint at `/api/gateway/openai/responses` that implements OpenAI's Responses API specification. This means you can use any OpenAI SDK (Python, JavaScript, Go, etc.) without modifying your code - just change the base URL.

## Usage Examples

<CodeGroup dropdown>
```python main.py
from openai import OpenAI

# Point the client to Uno's gateway endpoint
client = OpenAI(
    base_url="http://localhost:6060/api/gateway/openai",
    api_key="sk-amg-your-virtual-key-here",  # or your OpenAI API key
)

# Use the Responses API
response = client.responses.create(
    model="gpt-4.1-mini",
    instructions="You are a helpful assistant.",
    input="Hello, how are you?",
)

print(response.output_text)
```

```javascript main.js
const OpenAI = require('openai');

const openai = new OpenAI({
  baseURL: 'http://localhost:6060/api/gateway/openai',
  apiKey: 'sk-amg-your-virtual-key-here', // or your OpenAI API key
});

async function main() {
  const response = await openai.responses.create({
    model: 'gpt-4.1-mini',
    instructions: 'You are a helpful assistant.',
    input: 'Hello, how are you?',
  });

  console.log(response.output_text);
}

main();
```

```go main.go
package main

import (
    "context"
    "fmt"

    "github.com/openai/openai-go/v3"
    "github.com/openai/openai-go/v3/option"
)

func main() {
    client := openai.NewClient(
        option.WithBaseURL("http://localhost:6060/api/gateway/openai"),
        option.WithAPIKey("sk-amg-your-virtual-key-here"), // or your OpenAI API key
    )

    response, err := client.Responses.New(context.Background(), openai.ResponseNewParams{
        Model:       "gpt-4.1-mini",
        Instructions: "You are a helpful assistant.",
        Input:       "Hello, how are you?",
    })

    if err != nil {
        panic(err)
    }

    fmt.Println(response.OutputText)
}
```
</CodeGroup>

## Streaming Support

The gateway supports streaming responses via Server-Sent Events (SSE). Use your SDK's streaming methods as you normally would:

<CodeGroup dropdown>
```python streaming.py
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:6060/api/gateway/openai",
    api_key="sk-amg-your-virtual-key-here",
)

stream = client.responses.create(
    model="gpt-4.1-mini",
    instructions="You are a helpful assistant.",
    input="Tell me a story.",
    stream=True,
)

for chunk in stream:
    if chunk.output_text:
        print(chunk.output_text, end="", flush=True)
```

```javascript streaming.js
const OpenAI = require('openai');

const openai = new OpenAI({
  baseURL: 'http://localhost:6060/api/gateway/openai',
  apiKey: 'sk-amg-your-virtual-key-here',
});

async function main() {
  const stream = await openai.responses.create({
    model: 'gpt-4.1-mini',
    instructions: 'You are a helpful assistant.',
    input: 'Tell me a story.',
    stream: true,
  });

  for await (const chunk of stream) {
    if (chunk.output_text) {
      process.stdout.write(chunk.output_text);
    }
  }
}

main();
```
</CodeGroup>

## Supported Features

The gateway currently supports the Responses API with:
- ✅ **Text generation** - Standard text completions
- ✅ **Images** - Image generation and processing
- ✅ **Tool calling** - Function calling capabilities
- ✅ **Reasoning** - Advanced reasoning models

## Authentication

The gateway accepts authentication via the `Authorization` header with a Bearer token:

- **Virtual Key**: Use a virtual key (starts with `sk-amg-`) for managed access control
- **Direct API Key**: Use your OpenAI API key directly