---
title: Text Generation
description: Learn how to generate text using the Uno SDK, including unary and streaming responses.
---

The Text Generation API is the primary interface for interacting with LLMs through the Uno SDK. It provides a unified way to send prompts and receive responses from various providers (OpenAI, Anthropic, Gemini, etc.) using an unified structure.

## Getting Started

To use the Text Generation API, you first need to create an LLM model instance from your initialized Uno client.

```go
import (
    "github.com/praveen001/uno/pkg/gateway/sdk"
    "github.com/praveen001/uno/pkg/llm"
)

// Assuming 'client' is already initialized (Gateway or Direct mode)
model := client.NewLLM(sdk.LLMOptions{
    Provider: llm.ProviderNameOpenAI,
    Model:    "gpt-4o",
})
```

---

## Unary Requests

Unary requests are standard "request-response" calls where the SDK waits for the full completion from the LLM before returning.

### Example

```go
import (
    "context"
    "fmt"
    "github.com/praveen001/uno/pkg/llm/responses"
    "github.com/praveen001/uno/internal/utils"
)

func main() {
    // ... client and model initialization ...

    resp, err := model.NewResponses(context.Background(), &responses.Request{
        Instructions: utils.Ptr("You are a helpful assistant."),
        Input: responses.InputUnion{
            OfString: utils.Ptr("What is the capital of France?"),
        },
    })
    if err != nil {
        panic(err)
    }

    // Access the text output
    for _, output := range resp.Output {
        if output.OfOutputMessage != nil {
            for _, content := range output.OfOutputMessage.Content {
                if content.OfOutputText != nil {
                    fmt.Println(content.OfOutputText.Text)
                }
            }
        }
    }
}
```

---

## Streaming Responses

For real-time applications, the SDK supports streaming responses. This returns a channel that yields chunks of the response as they are generated by the LLM.

### Example

```go
import (
    "context"
    "fmt"
    "github.com/praveen001/uno/pkg/llm/responses"
    "github.com/praveen001/uno/internal/utils"
)

func main() {
    // ... client and model initialization ...

    stream, err := model.NewStreamingResponses(context.Background(), &responses.Request{
        Input: responses.InputUnion{
            OfString: utils.Ptr("Write a poem about coding."),
        },
    })
    if err != nil {
        panic(err)
    }

    for chunk := range stream {
        // Handle different types of chunks
        if chunk.OfOutputTextDelta != nil {
            fmt.Print(chunk.OfOutputTextDelta.Delta)
        }
    }
}
```

---

## Request Configuration

The `responses.Request` struct allows you to fine-tune your LLM calls with several parameters:

| Parameter | Type | Description |
| :--- | :--- | :--- |
| **Instructions** | `*string` | System-level instructions (System Prompt). |
| **Input** | `InputUnion` | The user prompt (string or list of messages). |
| **Temperature** | `*float64` | Sampling temperature (0.0 to 2.0). |
| **MaxOutputTokens** | `*int` | Limit on the number of tokens generated. |
| **TopP** | `*float64` | Nucleus sampling parameter. |
| **Tools** | `[]ToolUnion` | Definitions for function calling. |

### Using Multi-turn Conversation

Instead of a single string, you can pass a list of messages for context-aware conversations.

```go
resp, err := model.NewResponses(ctx, &responses.Request{
    Input: responses.InputUnion{
        OfInputMessageList: responses.InputMessageList{
            {
                OfEasyInput: &responses.EasyMessage{
                    Role:    "user",
                    Content: responses.EasyInputContentUnion{OfString: utils.Ptr("Hi!")},
                },
            },
            {
                OfEasyInput: &responses.EasyMessage{
                    Role:    "assistant",
                    Content: responses.EasyInputContentUnion{OfString: utils.Ptr("Hello! How can I help you?")},
                },
            },
            {
                OfEasyInput: &responses.EasyMessage{
                    Role:    "user",
                    Content: responses.EasyInputContentUnion{OfString: utils.Ptr("Tell me a joke.")},
                },
            },
        },
    },
})
```

