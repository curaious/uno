---
title: Text Generation
---

Uno SDK supports generating text with various LLM providers like OpenAI, Anthropic, and Gemini.

## Instantiate a model

You first need to create an LLM model instance from your initialized Uno client.

```go
model := client.NewLLM(sdk.LLMOptions{
    Provider: llm.ProviderNameOpenAI,
    Model:    "gpt-4o",
})
```

## Invoke the model

Using the model instance, you can invoke the model by passing various parameters like system instruction, temperature, topK etc.

```go
import (
    "context"
    "fmt"
    "github.com/praveen001/uno/pkg/llm/responses"
    "github.com/praveen001/uno/internal/utils"
)

func main() {
    // ... client and model initialization ...

    resp, err := model.NewResponses(context.Background(), &responses.Request{
        Instructions: utils.Ptr("You are a helpful assistant."),
        Input: responses.InputUnion{
            OfString: utils.Ptr("What is the capital of France?"),
        },
    })
    if err != nil {
        panic(err)
    }

    // Access the text output
    for _, output := range resp.Output {
        if output.OfOutputMessage != nil {
            for _, content := range output.OfOutputMessage.Content {
                if content.OfOutputText != nil {
                    fmt.Println(content.OfOutputText.Text)
                }
            }
        }
    }
}
```

---

## Streaming Responses

For real-time applications, the SDK supports streaming responses. This returns a channel that yields chunks of the response as they are generated by the LLM.

### Example

```go
import (
    "context"
    "fmt"
    "github.com/praveen001/uno/pkg/llm/responses"
    "github.com/praveen001/uno/internal/utils"
)

func main() {
    // ... client and model initialization ...

    stream, err := model.NewStreamingResponses(context.Background(), &responses.Request{
        Input: responses.InputUnion{
            OfString: utils.Ptr("Write a poem about coding."),
        },
    })
    if err != nil {
        panic(err)
    }

    for chunk := range stream {
        // Handle different types of chunks
        if chunk.OfOutputTextDelta != nil {
            fmt.Print(chunk.OfOutputTextDelta.Delta)
        }
    }
}
```

---

## Request Configuration

The `responses.Request` struct allows you to fine-tune your LLM calls with several parameters:

| Parameter | Type | Description |
| :--- | :--- | :--- |
| **Instructions** | `*string` | System-level instructions (System Prompt). |
| **Input** | `InputUnion` | The user prompt (string or list of messages). |
| **Temperature** | `*float64` | Sampling temperature (0.0 to 2.0). |
| **MaxOutputTokens** | `*int` | Limit on the number of tokens generated. |
| **TopP** | `*float64` | Nucleus sampling parameter. |
| **Tools** | `[]ToolUnion` | Definitions for function calling. |

## Using Multi-turn Conversation

Instead of a single string, you can pass a list of messages for context-aware conversations.

```go
resp, err := model.NewResponses(ctx, &responses.Request{
    Input: responses.InputUnion{
        OfInputMessageList: responses.InputMessageList{
            {
                OfEasyInput: &responses.EasyMessage{
                    Role:    "user",
                    Content: responses.EasyInputContentUnion{OfString: utils.Ptr("Hi!")},
                },
            },
            {
                OfEasyInput: &responses.EasyMessage{
                    Role:    "assistant",
                    Content: responses.EasyInputContentUnion{OfString: utils.Ptr("Hello! How can I help you?")},
                },
            },
            {
                OfEasyInput: &responses.EasyMessage{
                    Role:    "user",
                    Content: responses.EasyInputContentUnion{OfString: utils.Ptr("Tell me a joke.")},
                },
            },
        },
    },
})
```

